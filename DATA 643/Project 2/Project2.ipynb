{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Project 2: Content-Based and Collaborative Filtering\n",
    "By Latif Masud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Overview\n",
    "For this project, I broke it into two parts. The first part performs a Content-Based filering recommender system using data from OMDb's API, which then performed a tf-idf filtering. The data used was plots for all seven Harry Potter movies. The second part utlized a User-User Collaborative filtering recommender system overa  randomly generated set of user movie ratings. All the code work was done in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Content-Based Filtering\n",
    "For this part, OMDb API data was put through into `textBlob` and then calculations were performed.\n",
    "\n",
    "###Libraries\n",
    "The following libraries were used. `requests` was used to make API calls, wile `math` was used to perform tf-idf calcualations. `unicodedata` was used to convert unicode data that was returned by the API to a Python string. `textblob` was used to make it easy to figure out how much of each word was in a movie plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import unicodedata\n",
    "from textblob import TextBlob as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Functions\n",
    "The `tf` function returns the frequency of a word in a blob. `n_containing` returns the number of movie plots that contain the word. `idf` stands for Inverse Document Frequency, which is the calculation of how frequnently a word appears in a file, which is then inverted. `tfidf` is simply the product of `tf` and `idf`. Lastly, a `clean_words` function is defined to take out any very commonly occuring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return float(blob.words.count(word)) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return float(math.log(float(len(bloblist)) / (1 + n_containing(word, bloblist))))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return float(tf(word, blob)) * float(idf(word, bloblist))\n",
    "\n",
    "def clean_words (rm_words, query):\n",
    "    wrds = query.lower()\n",
    "    for word in rm_words:\n",
    "        wrds = wrds.replace(word, \"\")\n",
    "        \n",
    "    return wrds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Code\n",
    "A few common words such as \"harry\" and \"potter\" are taken out of the plots. This is done because the main character names would overwhelm the overall results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_words = [\"harry\", \"potter\", \"ron\", \"weasley\", \"hermione\", \"granger\", \"hogwarts\", \",\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array for all the plot summaries is created, and we also define an array of all the years that the movies came out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potter_summaries = []\n",
    "potter_years = [2011, 2010, 2009, 2007, 2005, 2004,2002, 2001]\n",
    "potter_titles = [\n",
    "    \"Harry Potter and the Deathly Hallows – Part 2\",\n",
    "    \"Harry Potter and the Deathly Hallows – Part 1\",\n",
    "    \"Harry Potter and the Half-Blood Prince\",\n",
    "    \"Harry Potter and the Order of the Phoenix\",\n",
    "    \"Harry Potter and the Goblet of Fire\",\n",
    "    \"Harry Potter and the Prisoner of Azkaban\",\n",
    "    \"Harry Potter and the Chamber of Secrets\",\n",
    "    \"Harry Potter and the Sorcerer's Stone\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the movie years, I make an API call to OMDb and get the result back. I then take the plot, convert it into a Python string from unicode. After that, all the words defined in `remove_words` are taken out, and appeneded into the summaries array as `TextBlob`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in potter_years:\n",
    "    r = requests.get('http://www.omdbapi.com/?apikey=ad492b8&t=harry+potter&y='+str(year)+'&plot=full')\n",
    "    plot = unicodedata.normalize('NFKD',  r.json()[\"Plot\"]).encode('ascii','ignore')\n",
    "    potter_summaries.append(tb(clean_words(remove_words, plot)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the plots, I run through a loop, and calculate the tf-idf scores for each word. For each file, I get back the top three words. The top three words are found by first sorting the `scores` array, and then reversing it. A for loop is then run for the first three spots of that sorted array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Harry Potter and the Deathly Hallows – Part 2\n",
      "\tWord: their, TF-IDF: 0.05231\n",
      "\tWord: mission, TF-IDF: 0.02616\n",
      "\tWord: battle, TF-IDF: 0.02616\n",
      "\n",
      "Top words in Harry Potter and the Deathly Hallows – Part 1\n",
      "\tWord: rest, TF-IDF: 0.04864\n",
      "\tWord: control, TF-IDF: 0.02432\n",
      "\tWord: trio, TF-IDF: 0.02432\n",
      "\n",
      "Top words in Harry Potter and the Half-Blood Prince\n",
      "\tWord: professor, TF-IDF: 0.04621\n",
      "\tWord: persuades, TF-IDF: 0.0231\n",
      "\tWord: slughorn, TF-IDF: 0.0231\n",
      "\n",
      "Top words in Harry Potter and the Order of the Phoenix\n",
      "\tWord: ca, TF-IDF: 0.02773\n",
      "\tWord: n't, TF-IDF: 0.01962\n",
      "\tWord: that, TF-IDF: 0.0141\n",
      "\n",
      "Top words in Harry Potter and the Goblet of Fire\n",
      "\tWord: three, TF-IDF: 0.01874\n",
      "\tWord: magical, TF-IDF: 0.01874\n",
      "\tWord: goblet, TF-IDF: 0.01766\n",
      "\n",
      "Top words in Harry Potter and the Prisoner of Azkaban\n",
      "\tWord: using, TF-IDF: 0.01912\n",
      "\tWord: will, TF-IDF: 0.01353\n",
      "\tWord: he, TF-IDF: 0.0119\n",
      "\n",
      "Top words in Harry Potter and the Chamber of Secrets\n",
      "\tWord: terrible, TF-IDF: 0.02888\n",
      "\tWord: dobby, TF-IDF: 0.02888\n",
      "\tWord: gets, TF-IDF: 0.02888\n",
      "\n",
      "Top words in Harry Potter and the Sorcerer's Stone\n",
      "\tWord: an, TF-IDF: 0.02411\n",
      "\tWord: quickly, TF-IDF: 0.02411\n",
      "\tWord: learns, TF-IDF: 0.01706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(potter_summaries):\n",
    "    print(\"Top words in \" + potter_titles[i])\n",
    "    scores = {word: tfidf(word, blob, potter_summaries) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##User-User Collaborative Filtering\n",
    "For this portion, a random matrix for five users and ten movies is first generated, and then based on those ratings, recommendations are made based on regression and gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Libraries\n",
    "For this section, only two libraries are needed: `numpy` for defining matrices, and performing some of the regression, and `scipy`'s optimize to make complex regressions easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`normalize_ratings` takes in two matrices: the ratings and if the user did rate the movie. Based on this data, the mean is calculated of the ratings, and then normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_ratings(ratings, did_rate):\n",
    "    num_movies = ratings.shape[0]\n",
    "    \n",
    "    ratings_mean = zeros(shape = (num_movies, 1))\n",
    "    ratings_norm = zeros(shape = ratings.shape)\n",
    "    \n",
    "    for i in range(num_movies): \n",
    "        idx = where(did_rate[i] == 1)[0]\n",
    "        \n",
    "        #  Calculate mean \n",
    "        ratings_mean[i] = mean(ratings[i, idx])\n",
    "        ratings_norm[i, idx] = ratings[i, idx] - ratings_mean[i]\n",
    "    \n",
    "    return ratings_norm, ratings_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unroll_params` calculates the shapes based on movies and features. It takes in data, the number of users, movies, and features, and then defines a matrix for the number of movies and features. A matrix transposition is then perofrmed. The same is done for the number of features and users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unroll_params(X_and_theta, num_users, num_movies, num_features):\n",
    "    m = X_and_theta[:num_movies * num_features]\n",
    "    x = m.reshape((num_features, num_movies)).transpose()\n",
    "    y = X_and_theta[num_movies * num_features:]\n",
    "    theta = y.reshape(num_features, num_users ).transpose()\n",
    "    return x, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calcualte_gradient` calcualates the gradient descent of the variables. We start by unrolling the parms, then calculating the difference of the dot product of x, theta, and the did_rate matrix. This returns the intial predictions, and then we subtract the ratings from it. Finally, the x and theta gradients are produced (did not fully understand the math behind the last two lines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(X_and_theta, ratings, did_rate, num_users, num_movies, num_features, reg_param):\n",
    "    X, theta = unroll_params(X_and_theta, num_users, num_movies, num_features)\n",
    "\n",
    "    # we multiply by did_rate because we only want to consider observations for which a rating was given\n",
    "    difference = X.dot( theta.T ) * did_rate - ratings\n",
    "    X_grad = difference.dot( theta ) + reg_param * X\n",
    "    theta_grad = difference.T.dot( X ) + reg_param * theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_cost` returns the total sum of the squared errors of the data that is pushed in. First, the parms are unrolled, then the cost (sum) is calculated, and then it is regularazed, which is getting the error of the sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cost(X_and_theta, ratings, did_rate, num_users, num_movies, num_features, reg_param):\n",
    "    X, theta = unroll_params(X_and_theta, num_users, num_movies, num_features)\n",
    "\n",
    "    cost = sum( (X.dot( theta.T ) * did_rate - ratings) ** 2 ) / 2\n",
    "    regularization = (reg_param / 2) * (sum( theta**2 ) + sum(X**2))\n",
    "    return cost + regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we define the number of users and movies we want to use for our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_movies = 10\n",
    "num_users = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, a `numpy` random array/matrix is defined of the shape of the two parms defined above and a max value of 10: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = random.randint(11, size= (num_movies, num_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For personalized ratings for me, I first define a matrix of all zeroes, and then manually plug in a few values, and append it to our existing ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latif_ratings = zeros((num_movies, 1))\n",
    "\n",
    "latif_ratings[0] = 8\n",
    "latif_ratings[4] = 8\n",
    "latif_ratings[7] = 3\n",
    "\n",
    "ratings = append(latif_ratings, ratings, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings matrix looks like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  8.  10.   7.   5.   0.   0.]\n",
      " [  0.   3.   3.   4.   4.   9.]\n",
      " [  0.   5.   6.   4.   7.   1.]\n",
      " [  0.   5.   9.   4.   4.   4.]\n",
      " [  8.   8.   2.   4.   6.   8.]\n",
      " [  0.   5.   4.   4.   0.   9.]\n",
      " [  0.   7.   5.   0.   3.   1.]\n",
      " [  3.   8.   9.  10.  10.   4.]\n",
      " [  0.   6.   7.   7.   1.   5.]\n",
      " [  0.   5.   2.   6.   4.   2.]]\n"
     ]
    }
   ],
   "source": [
    "print ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A did rate function is defined, which is simply represents boolean values of whether or not a user rated a movie, which is represented by a value greater than 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "did_rate = (ratings != 0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then normalize our ratings, update the number of users (since this changes when I add my personal values in), and set a number of features. These features can be things such as \"Comedy\", \"Action\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings, ratings_mean = normalize_ratings(ratings, did_rate)\n",
    "num_users = ratings.shape[1]\n",
    "num_features = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix of random movie features and user preferences are then generated, and a row-wise merge (the `r_` part) is performed to create a matrix that has the features and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_features = random.randn( num_movies, num_features )\n",
    "user_prefs = random.randn( num_users, num_features )\n",
    "initial_X_and_theta = r_[movie_features.T.flatten(), user_prefs.T.flatten()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the minimized cost is calculated by plugging in variables into the `scipy` optimize function. fmin_cg minimizes using nonlinear gradients, and a maximum iternation (used for accuray) of 100 is set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-abc17548fcda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_X_and_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdid_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_movies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     maxiter=100, disp=True, full_output=True)\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Latif Masud\\Anaconda2\\lib\\site-packages\\scipy\\optimize\\optimize.pyc\u001b[0m in \u001b[0;36mfmin_cg\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[0;32m   1175\u001b[0m             'return_all': retall}\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latif Masud\\Anaconda2\\lib\\site-packages\\scipy\\optimize\\optimize.pyc\u001b[0m in \u001b[0;36m_minimize_cg\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m   1232\u001b[0m     \u001b[1;31m# Sets the initial step guess to dx ~ 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m     \u001b[0mold_fval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m     \u001b[0mold_old_fval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mold_fval\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgfk\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretall\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Latif Masud\\Anaconda2\\lib\\site-packages\\numpy\\linalg\\linalg.pyc\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2173\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2174\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2175\u001b[1;33m                 \u001b[0msqnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2176\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "reg_param = 30\n",
    "\n",
    "minimized_cost_and_optimal_params = optimize.fmin_cg(\n",
    "    calculate_cost, \n",
    "    fprime=calculate_gradient, \n",
    "    x0=initial_X_and_theta,\n",
    "    args=(ratings, did_rate, num_users, num_movies, num_features, reg_param),\n",
    "    maxiter=100, disp=True, full_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we set the cost and optimal features for just my values, and then calculate all the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minimized_cost_and_optimal_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-aa3a43fc6a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimal_movie_features_and_user_prefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimized_cost_and_optimal_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminimized_cost_and_optimal_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmovie_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_prefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munroll_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimal_movie_features_and_user_prefs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_movies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mall_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmovie_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0muser_prefs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'minimized_cost_and_optimal_params' is not defined"
     ]
    }
   ],
   "source": [
    "cost, optimal_movie_features_and_user_prefs = minimized_cost_and_optimal_params[1], minimized_cost_and_optimal_params[0]\n",
    "\n",
    "movie_features, user_prefs = unroll_params(optimal_movie_features_and_user_prefs, num_users, num_movies, num_features)\n",
    "all_predictions = movie_features.dot( user_prefs.T )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I pull out my recommendations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-980a94b9fc01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions_for_latif\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mratings_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mpredictions_for_latif\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_for_latif = all_predictions[:, 0:1] + ratings_mean\n",
    "print predictions_for_latif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: The last few parts of the script did not work on Jupyter. The script is included in the GitHub repo, and can be run error free. \n",
    "\n",
    "The values generated for random ratings generated for one run are below: \n",
    "*Ratings*:\n",
    "[[  8.   5.   3.   7.   5.   8.]\n",
    " [  0.   0.  10.   8.   8.   7.]\n",
    " [  0.   8.  10.   8.   3.   1.]\n",
    " ..., \n",
    " [  3.   3.   4.  10.   9.   2.]\n",
    " [  0.   9.   0.   8.   9.   6.]\n",
    " [  0.   0.  10.   5.   8.   0.]]\n",
    " \n",
    " *Predictions for Latif*:\n",
    " [[ 6.        ]\n",
    " [ 8.25      ]\n",
    " [ 6.        ]\n",
    " [ 8.75      ]\n",
    " [ 7.4       ]\n",
    " [ 6.2       ]\n",
    " [ 4.6       ]\n",
    " [ 5.16666667]\n",
    " [ 8.        ]\n",
    " [ 7.66666667]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tough assignment. The toughest part was often that the math exceeded my current knowledge level. With that said, the second part of the project- the User-User filtering mechanism- was successfully able to recommend movie ratings for me based on randomized data. The first portion was tougher to make predictions using since most of the top words were always adjectives or common words. My initial goal was to predict the sub-vilian of each movie (since the ultimate villain is always Voldemort) based on how often they appear in the plot, but that unfornuately was not possible as the villian's name did not appear very often. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
